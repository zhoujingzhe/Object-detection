{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "Thie file is to train the model.    \n",
    "<span style=\"color:red\">Warning: our model is trained on multi-gpu server, if you don't have multi-gpu on your server, you should cancel the two lines, 'model = multi_gpu_model(model, gpus=4)'</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "import xml.etree.ElementTree as ET\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.utils import multi_gpu_model\n",
    "import pydot\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from datetime import datetime\n",
    "import os\n",
    "from settings import setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data\n",
    "LOAD_IMAGE is a tool function for loading the image into binary form.  \n",
    "readXML is a tool function for reading the label information with respect to a image.  \n",
    "load_DATA is to load all images into a path.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOAD_IMAGE(path):\n",
    "    image1 = load_img(path)\n",
    "    train_example = img_to_array(image1, data_format='channels_first')\n",
    "    img = array_to_img(train_example, data_format='channels_first')\n",
    "    train_example = train_example.transpose()\n",
    "    return train_example\n",
    "\n",
    "def readXML(f):\n",
    "    mapped = {}\n",
    "    tree = ET.parse(f)\n",
    "    root = tree.getroot()\n",
    "    for elem in root:\n",
    "        if len(elem) == 0:\n",
    "            mapped[elem.tag] = elem.text\n",
    "        for subelem in elem:\n",
    "            if len(subelem) == 0:\n",
    "                mapped[subelem.tag] = subelem.text\n",
    "            for sub2elem in subelem:\n",
    "                if len(sub2elem) == 0:\n",
    "                    mapped[sub2elem.tag] = sub2elem.text\n",
    "    xmax = int(mapped['xmax'])\n",
    "    xmin = int(mapped['xmin'])\n",
    "    ymax = int(mapped['ymax'])\n",
    "    ymin = int(mapped['ymin'])\n",
    "    C_index = int(mapped['name'][1])\n",
    "    yTrue = np.zeros((19, 14, 7), dtype='float32')\n",
    "    yTrue[:, :] = [xmax, xmin, ymax, ymin, 0, 0, C_index]\n",
    "    return yTrue\n",
    "\n",
    "def load_DATA(srcDir):\n",
    "    fileCount = len([name for name in os.listdir(srcDir) if name.endswith(\".xml\")])\n",
    "    train_data = np.empty(shape=[fileCount, 640, 480, 3], dtype='float32')\n",
    "    train_label = np.empty(shape=[fileCount, 19, 14, 7], dtype='float32')\n",
    "    count = 0\n",
    "    for filename in os.listdir(srcDir):\n",
    "        if not filename.endswith(\".xml\"): continue\n",
    "        count += 1\n",
    "        xmlFile = srcDir + \"/\" + filename\n",
    "        print(xmlFile)\n",
    "        imgFile = xmlFile.replace(\".xml\", \".jpg\")\n",
    "        print(imgFile)\n",
    "        train_data[count - 1] = LOAD_IMAGE(imgFile)\n",
    "        train_label[count - 1] = readXML(xmlFile)\n",
    "    train_label[:, :, :, 4] = np.arange(0, 19, 1).reshape(19, 1)\n",
    "    train_label[:, :, :, 5] = np.arange(0, 14, 1).reshape(1, 14)\n",
    "    return [train_data, train_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function is loading corresponding networks with specific loss function according to setting parameters\n",
    "\n",
    "A setting format is like:  \n",
    "    \"setting1\": {  \n",
    "        \"loss\": \"XXX\",  \n",
    "        \"architecture\": \"XXX\",  \n",
    "        \"model\": \"DecayByBatch\",    \n",
    "        \"weight_Classification_loss\": X,  \n",
    "        \"weight_Object_loss\": X,  \n",
    "        \"weight_Localization_loss\": X,  \n",
    "        \"lr\": XXX,  \n",
    "        \"decay\": XXX,  \n",
    "        \"weight_file\": \"weight-settingX\",  \n",
    "        \"loss_file\": \"losses-settingX.txt\",  \n",
    "        \"batch_size\": XXX,  \n",
    "        \"epochs\": XXX  \n",
    "    }  \n",
    "Thus, with a setting parameter, the main function will automatically build up or load a model on this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initModel is a sign for if we already have a trained model stored in .h5 file.\n",
    "def main(iniModel, setting):      \n",
    "    # Choosing different architecture\n",
    "    if setting[\"architecture\"] == \"Yolo_V1\":\n",
    "        import Yolo_V1 as yolo\n",
    "        import Util_V1 as U     \n",
    "    elif setting[\"architecture\"] == \"Yolo_V2\":\n",
    "        import Yolo_V2 as yolo\n",
    "        import Util_V2 as U\n",
    "        \n",
    "        \n",
    "    # Choosing different decay mechanism\n",
    "    if setting['model'] == \"DecayByBatch\":\n",
    "        optimizer = Adam(lr=setting[\"lr\"])\n",
    "        CallBackFun = U.lr_minimum()\n",
    "    elif setting['model'] == \"DecayByEpoch\":\n",
    "        optimizer = Adam(lr=setting[\"lr\"])\n",
    "        CallBackFun = U.DecayByEpoch()\n",
    "    \n",
    "    # Choosing different loss function\n",
    "    if setting['loss'] == 'Loss_v2':\n",
    "        lossFunction = U.Loss_v2\n",
    "    elif setting['loss'] == 'Loss_v3':\n",
    "        lossFunction = U.Loss_v3  \n",
    "    \n",
    "    # folderCount is a variable whose value represents how many folders the model are loading\n",
    "    # each folder has 1000 images with .xml files - deleted\n",
    "    # folder 0 has 20,000 images with .xml files\n",
    "    # folder 1 has 2,410 images with .xml files \n",
    "    # isDone is a variable that represents if the model has been loaded, to avoid repeatedly loading\n",
    "    folderCount = 1\n",
    "    isDone = False\n",
    "    for count in range(0, folderCount, 1):\n",
    "        srcDir = \"src/\" + count \n",
    "        print(\"+++ run: \"+ srcDir + \" \" + str(datetime.now()) + \"+++\")\n",
    "        train_data, train_label = load_DATA(srcDir)\n",
    "        \n",
    "        # to transform the class representation into one-hot encoding for cross-entropy loss  \n",
    "        one_hot_encoding = to_categorical(y=train_label[:, :, :, 6], num_classes=10)\n",
    "        train_label = np.concatenate((train_label, one_hot_encoding), axis = -1)\n",
    "        \n",
    "        # if we don't have a existed model file .h5, we should initialize a model\n",
    "        if iniModel:\n",
    "            model = yolo.network_architecture(input_data=[640, 480, 3])\n",
    "            model = multi_gpu_model(model, gpus=4)\n",
    "            model.compile(optimizer=optimizer, loss=lossFunction)\n",
    "            iniModel = False\n",
    "            plot_model(model=model, to_file='Architecture.png', show_layer_names=False)\n",
    "            SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "        \n",
    "        # well, if we already trained a model before, we should load the existed .h5 file \n",
    "        # but when we have a model running, we should not load again\n",
    "        elif not isDone:\n",
    "            model = load_model(setting[\"weight_file\"], custom_objects={setting[\"loss\"]: lossFunction})\n",
    "            model = multi_gpu_model(model, gpus=4)\n",
    "        isDone = True\n",
    "        \n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(setting[\"weight_file\"]+'-{epoch:08d}.h5', save_weights_only=True, period=1)\n",
    "        history = model.fit(x=train_data, y=train_label, validation_split=0.20, batch_size=setting[\"batch_size\"], \n",
    "                            epochs=setting[\"epochs\"], callbacks=[CallBackFun, checkpoint])\n",
    "        \n",
    "        # saving a log in case the exception occurs\n",
    "        f = open(setting[\"loss_file\"], \"a+\")\n",
    "        f.write(\"\\n\")\n",
    "        description = \"+++ run: \"+ srcDir + \" \" + str(datetime.now()) + \"+++\"\n",
    "        f.write(description)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(history.history))\n",
    "        f.close()\n",
    "        print(\"+++ saved: \" + str(datetime.now()) + \"+++\")\n",
    "        model.save(setting[\"weight_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+++ start: \" + str(datetime.now()) + \"+++\")\n",
    "iniModel = True\n",
    "if os.path.isfile(setting[\"weight_file\"]):\n",
    "    iniModel = False\n",
    "main(iniModel, setting)\n",
    "print(\"+++ finished: \" + str(datetime.now()) + \"+++\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
