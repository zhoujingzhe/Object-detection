{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture for object detection\n",
    "This file is to build up a neural network for processing the images to find out the information of objects' behaviors within this images, not only the classes, but also location.  \n",
    "Firstly import the packages we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from keras.layers import Input, Add, LeakyReLU, Activation, concatenate, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "import xml.etree.ElementTree as ET\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "#%matplotlib inline\n",
    "import Util_V1 as U\n",
    "import os\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intuition behind inception architecture\n",
    "Like the picture below, the one input is transferring to four different paths to generate feauture maps, which implicts that instead of using only one filter to extract feautures, the complex of four different feature extractors is able to excavate internal information in four different views. This is more stable and reliable than counting on one filter.  \n",
    "<img src=\"figures/Inception_Module_self.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 5px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(X, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "\n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    Conv_name_base = block + '_branch'\n",
    "    BN_name_base = block + '_branch'\n",
    "    F1, F2, F3, F4 = filters\n",
    "    print(X)\n",
    "\n",
    "    X1 = Conv2D(filters=F1, kernel_size=(1, 1), padding='valid', strides=(1, 1), name=Conv_name_base + 'Conv1a',\n",
    "                kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X1 = BatchNormalization(axis=1, name=BN_name_base + 'Batch1a')(X1)\n",
    "    X1 = Activation('relu')(X1)\n",
    "\n",
    "    X2 = Conv2D(filters=F2, kernel_size=(1, 1), padding='valid', strides=(1, 1), name=Conv_name_base + 'Conv1b',\n",
    "                kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X2 = BatchNormalization(axis=1, name=BN_name_base + 'Batch1b')(X2)\n",
    "    X2 = Activation('relu')(X2)\n",
    "    X2 = Conv2D(filters=F2, kernel_size=(3, 3), padding='same', strides=(1, 1), name=Conv_name_base + 'Conv2b',\n",
    "                kernel_initializer=glorot_uniform(seed=0))(X2)\n",
    "    X2 = BatchNormalization(axis=1, name=BN_name_base + 'Batch2b')(X2)\n",
    "    X2 = Activation('relu')(X2)\n",
    "\n",
    "    X3 = Conv2D(filters=F3, kernel_size=(1, 1), padding='valid', strides=(1, 1), name=Conv_name_base + 'Conv1c',\n",
    "                kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X3 = BatchNormalization(axis=1, name=BN_name_base + 'Batch1c')(X3)\n",
    "    X3 = Activation('relu')(X3)\n",
    "    X3 = Conv2D(filters=F3, kernel_size=(5, 5), padding='same', strides=(1, 1), name=Conv_name_base + 'Conv2c',\n",
    "                kernel_initializer=glorot_uniform(seed=0))(X3)\n",
    "    X3 = BatchNormalization(axis=1, name=BN_name_base + 'Batch2c')(X3)\n",
    "    X3 = Activation('relu')(X3)\n",
    "\n",
    "    X4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(X)\n",
    "    X4 = Conv2D(filters=F4, kernel_size=(1, 1), padding='valid', strides=(1, 1), name=Conv_name_base + 'Conv1d',\n",
    "                kernel_initializer=glorot_uniform(seed=0))(X4)\n",
    "    X4 = BatchNormalization(axis=1, name=BN_name_base + 'Batch1d')(X4)\n",
    "    X4 = Activation('relu')(X4)\n",
    "\n",
    "    X = concatenate([X1, X2, X3, X4],  axis=-1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intuition behind ResNet\n",
    "We put two shortcuts in this architecture. The reasons are not only we hope to avoid gradient vanishing problem, but also we consider if there is a shortcut linking the layers in the front and back, by the shortcut, the layer in the front can make a big stride toward the optimal point rather than a small step. That will reduce the cost on time and computations.  \n",
    " \n",
    "<img src=\"figures/Resnet.PNG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 5px;\" />    \n",
    "       \n",
    "For an example, in backpropagation, the gradient in layer i directly pass to layer 1. It results in a big progress in layer 1 and it speeds up the layer 1 converging to optimal states. Therefore, compared with conventional linear backpropogation, in our opinion, adding shortcuts is a good way to accelerate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_architecture(input_data):\n",
    "    \"\"\"\n",
    "    Implementation of the HappyModel.\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    X_shortcut_1 = 0\n",
    "    X_shortcut_2 = 0\n",
    "    Input_data = Input(input_data)  # 640x480\n",
    "    X = Conv2D(filters=32, kernel_size=(8, 8), strides=2, kernel_initializer=glorot_uniform(seed=0), padding='valid')(\n",
    "        Input_data)  # 317x237\n",
    "    X_shortcut_1 = X\n",
    "    X = BatchNormalization(axis=1)(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "    X = MaxPooling2D(pool_size=(3, 3), strides=2, padding='valid')(X)  # 158x118\n",
    "\n",
    "    X = inception_block(X=X, filters=[64, 32, 32, 64], block='inception', stage=1)  # 158x118\n",
    "    X_shortcut_2 = X\n",
    "    X = BatchNormalization(axis=1)(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "\n",
    "    X = Conv2D(filters=128, kernel_size=(2, 2), strides=2, padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)  # 79x59\n",
    "    X = BatchNormalization(axis=1)(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "\n",
    "    X_shortcut_1 = Conv2D(filters=128, kernel_size=(5, 5), strides=4, padding='valid', kernel_initializer=glorot_uniform(seed=0))(X_shortcut_1)\n",
    "    X_shortcut_1 = BatchNormalization(axis=1)(X_shortcut_1)\n",
    "    X_shortcut_1 = LeakyReLU(alpha=0.3)(X_shortcut_1)\n",
    "\n",
    "    X = Add()([X_shortcut_1, X])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=256, kernel_size=(3, 3), strides=2, padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)  # 39x29\n",
    "    X = BatchNormalization(axis=1)(X)\n",
    "    X = LeakyReLU(alpha=0.3)(X)\n",
    "\n",
    "    X_shortcut_2 = Conv2D(filters=256, kernel_size=(6, 6), strides = 4, padding='valid', kernel_initializer=glorot_uniform(seed=0))(X_shortcut_2)\n",
    "    X_shortcut_2 = BatchNormalization(axis= 1)(X_shortcut_2)\n",
    "    X_shortcut_2 = LeakyReLU(alpha=0.3)(X_shortcut_2)\n",
    "\n",
    "    X = concatenate([X_shortcut_2, X], axis=-1)\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(3, 3), strides=2, padding='valid')(X)  # 19x19\n",
    "    X = Conv2D(filters=15, kernel_size=(1, 1), strides=1, padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)  # 19x14\n",
    "    model = Model(inputs=Input_data, outputs=X, name='SimpleYOLO')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final Architecture\n",
    "To sum up, we eventually devise an network architecture like this.\n",
    "<img src=\"figures/Architecture_YOLOV1.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 5px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
